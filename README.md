# Healthy and Defective Fruits Classification (RGB + SAM/K-Means Silhouette Fusion)

**Goal:** classify apple images as **healthy vs defective** by fusing two inputs per sample:
- **RGB image** (texture / color cues)
- **Silhouette / mask image** (shape + defect-region cues)

The silhouette is generated by a lightweight **SAM + K-means anomaly** pipeline:
1) **SAM** segments the fruit (object silhouette)
2) K-means clustering on robust color/texture features inside the fruit detects **defect pixels**
3) Final binary mask keeps **healthy fruit = white**, while **defects + background = black**


---

## Abstract
We propose a lightweight multi-input fruit-quality classifier that combines RGB appearance with a segmentation-derived silhouette channel. The silhouette is produced using a two-stage pipeline: Segment Anything Model (SAM) isolates the fruit, and a K-means–based anomaly detector identifies defect regions using robustly normalized LAB/HSV/gradient features. Two MobileNetV2 encoders extract features from RGB and silhouette inputs, which are fused by a learnable gate and classified with an MLP head. The training pipeline supports paired-file matching by ID-stem, handles class imbalance via weighted sampling, and reports accuracy and a confusion matrix per epoch. We provide scripts to generate publication-style figures from training logs and datasets.

---

## 1) Method overview

### 1.1 Silhouette / defect generation (SAM + K-means anomaly)
**Input:** RGB image

**Step 1 — SAM object mask (fruit silhouette):**
- Prompt: center positive point (+ optional corner negative points)
- Select best mask while rejecting corner-touching and extreme-area masks
- Morphological close/open to smooth the silhouette

**Step 2 — Defect detection via K-means anomaly (inside fruit only):**
- Features per fruit pixel: **LAB (L,A,B) + HSV saturation (S) + gradient magnitude**
- Robust normalization: median / MAD
- K-means clustering over fruit pixels
- Anomaly candidates = **(rare clusters)** OR **(distance-to-center tail percentile)**
- Morphological cleanup + remove small components
- Gates suppress false positives on healthy fruit:
  - min defect ratio
  - min anomaly separation

**Output:** final binary `*_step3_binary.png`  
- healthy fruit pixels = 255 (white)  
- defects + background = 0 (black)

> **Figure 1 (placeholder): SAM + K-means silhouette/defect pipeline**
>
> `assets/fig1_sam_kmeans_pipeline.png`  
> Generate with:
> ```bash
> python scripts/plot_flow_diagram.py --out assets/fig1_sam_kmeans_pipeline.png
> ```

> **Figure 2 (placeholder): Example outputs (RGB + generated step3 binary)**
>
> `assets/fig2_sam_kmeans_examples.png`  
> Generate with:
> ```bash
> python scripts/plot_sam_kmeans_examples.py --rgb_dir images/fresh --mask_dir outputs_step3_only_fresh --out assets/fig2_sam_kmeans_examples.png
> ```

### 1.2 Multi-input classifier (RGB + silhouette)
**Inputs:** (RGB, silhouette) → **two encoders** → **fusion** → **classifier**

- Backbone: MobileNetV2 feature extractor (ImageNet-pretrained optional)
- Fusion: `gated` (default) / `concat` / `sum`
- Head: MLP (Linear → ReLU → Dropout → Linear)

> **Figure 3 (placeholder): Model overview**
>
> `assets/fig3_model_overview.png` (optional; you can add later)

---

## 2) Dataset format

### 2.1 Folder layout (required)
Put your classification dataset under `./images/`:

```
images/
  fresh/
  fresh_silhouette/
  rot_defect/
  rot_defect_silhouette/
  bruise_defect/
  bruise_defect_silhouette/
  scab_defect/
  scab_defect_silhouette/
```

### 2.2 Pairing rule (important)
Files are paired by **ID-stem**. Example:

- RGB: `SD_REAL_0001.jpg` → stem `SD_REAL_0001`
- Silhouette: `SD_REAL_0001_step3_binary.png` → base stem `SD_REAL_0001`

Silhouette files must contain the suffix **`_step3_binary`**.

> **Figure 4 (placeholder): Paired training samples (RGB + silhouette)**
>
> `assets/fig4_paired_training_examples.png`  
> Generate with:
> ```bash
> python scripts/plot_dataset_examples.py --images_root images --out assets/fig4_paired_training_examples.png
> ```

---

## 3) Training

### 3.1 Environment
```bash
conda create -n fruit_cls python=3.11 -y
conda activate fruit_cls
pip install -r requirements.txt
```

### 3.2 Train
```bash
python train.py | tee results/train.log
```

Training prints:
- train loss / accuracy
- val loss / accuracy
- confusion matrix: `[[TN, FP], [FN, TP]]`

A best checkpoint is saved to:
- `results/best_model.pt`

---

## 4) Results (placeholders)

> **Figure 5 (placeholder): Loss curve**
>
> `assets/fig5_loss_curve.png`

> **Figure 6 (placeholder): Accuracy curve**
>
> `assets/fig6_accuracy_curve.png`

> **Figure 7 (placeholder): Best-epoch confusion matrix**
>
> `assets/fig7_confusion_matrix_best.png`

Generate these after training:

```bash
python scripts/plot_training_from_log.py --log results/train.log --out assets
```

Optional: regenerate confusion matrix from checkpoint (runs eval again):
```bash
python scripts/eval_and_plot_cm.py --ckpt results/best_model.pt --images_root images --out assets
```

---

## 5) Reproducibility & engineering notes
- Deterministic seeds are set for CPU/GPU for repeatable runs.
- Class imbalance can be mitigated via `WeightedRandomSampler`.
- Both RGB and silhouettes are normalized using ImageNet stats (good default for pretrained MobileNetV2).

---

## 6) Silhouette pipeline code (SAM + K-means)
The silhouette/defect generation is implemented as a batch pipeline that:
- loads a SAM checkpoint once
- runs segmentation + anomaly-based defect extraction for all images in a folder
- writes only `*_step3_binary.png` outputs (healthy fruit white)

> You can place the provided pipeline script under `scripts/` (e.g., `scripts/run_sam_kmeans_batch.py`)  
> and run it to generate silhouette folders for training.

---

## Suggested citation (optional)
```bibtex
@misc{wang_fruit_sam_kmeans_fusion_2026,
  author       = {Ziyuan Wang},
  title        = {Healthy and Defective Fruits Classification via RGB + SAM/K-Means Silhouette Fusion},
  year         = {2026},
  howpublished = {GitHub repository}
}
```

---

## License
Add a license before sharing publicly (MIT / Apache-2.0 are common).
